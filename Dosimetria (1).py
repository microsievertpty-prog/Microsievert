# -*- coding: utf-8 -*-
import io
import re
import json
import requests
import pandas as pd
import streamlit as st
from datetime import datetime
from io import BytesIO
from typing import List, Dict, Any, Optional

# ===================== NINOX CONFIG =====================
API_TOKEN   = "edf312a0-98b8-11f0-883e-db77626d62e5"
TEAM_ID     = "YrsYfTegptdZcHJEj"
DATABASE_ID = "ow1geqnkz00e"
BASE_URL    = "https://api.ninox.com/v1"

# Tablas por NOMBRE (se resuelven a ID autom√°ticamente)
TABLE_LISTA = "LISTA DE CODIGO"     # lectura (personas / c√≥digos / periodo)
TABLE_BASE  = "BASE DE DATOS"       # escritura y reporte

# ===================== STREAMLIT APP =====================
st.set_page_config(page_title="Microsievert ‚Äî Dosimetr√≠a", page_icon="üß™", layout="wide")
st.title("üß™ Carga y Cruce de Dosis ‚Üí Ninox (BASE DE DATOS)")

# --------- Estado
if "df_lista" not in st.session_state:
    st.session_state.df_lista = None
if "df_dosis" not in st.session_state:
    st.session_state.df_dosis = None

# ===================== Utilidades =====================
def ninox_headers():
    return {"Authorization": f"Bearer {API_TOKEN}", "Content-Type": "application/json"}

def ninox_list_tables(team_id: str, db_id: str):
    url = f"{BASE_URL}/teams/{team_id}/databases/{db_id}/tables"
    r = requests.get(url, headers=ninox_headers(), timeout=30)
    r.raise_for_status()
    return r.json()

def ninox_resolve_table_id(team_id: str, db_id: str, table_hint: str) -> str:
    hint = (table_hint or "").strip()
    # Si parece ID (corto y sin espacios), se usa tal cual
    if hint and " " not in hint and len(hint) <= 8:
        return hint
    for t in ninox_list_tables(team_id, db_id):
        name = str(t.get("name","")).strip().lower()
        tid  = str(t.get("id","")).strip()
        if name == hint.lower() or tid == hint:
            return tid
    return hint

def ninox_fetch_all(table_hint: str, page_size: int = 1000) -> List[dict]:
    table_id = ninox_resolve_table_id(TEAM_ID, DATABASE_ID, table_hint)
    url = f"{BASE_URL}/teams/{TEAM_ID}/databases/{DATABASE_ID}/tables/{table_id}/records"
    out, skip = [], 0
    while True:
        r = requests.get(url, headers=ninox_headers(), params={"limit": page_size, "skip": skip}, timeout=60)
        r.raise_for_status()
        chunk = r.json()
        if not chunk: break
        out.extend(chunk)
        if len(chunk) < page_size: break
        skip += page_size
    return out

def ninox_insert_rows(table_hint: str, rows: List[dict], batch_size: int = 300) -> Dict[str, Any]:
    table_id = ninox_resolve_table_id(TEAM_ID, DATABASE_ID, table_hint)
    url = f"{BASE_URL}/teams/{TEAM_ID}/databases/{DATABASE_ID}/tables/{table_id}/records"
    if not rows:
        return {"ok": True, "inserted": 0}
    inserted = 0
    for i in range(0, len(rows), batch_size):
        chunk = rows[i:i+batch_size]
        r = requests.post(url, headers=ninox_headers(), json=chunk, timeout=60)
        if r.status_code != 200:
            return {"ok": False, "inserted": inserted, "error": f"{r.status_code} {r.text}"}
        inserted += len(chunk)
    return {"ok": True, "inserted": inserted}

def ninox_get_fields(table_hint: str) -> set:
    table_id = ninox_resolve_table_id(TEAM_ID, DATABASE_ID, table_hint)
    info = ninox_list_tables(TEAM_ID, DATABASE_ID)
    fields = set()
    for t in info:
        if str(t.get("id")) == str(table_id):
            cols = t.get("fields") or t.get("columns") or []
            for c in cols:
                nm = c.get("name") if isinstance(c, dict) else None
                if nm: fields.add(nm)
            break
    return fields

def as_num(v) -> float:
    if v is None: return 0.0
    s = str(v).strip().replace(",", ".")
    if s == "" or s.upper() == "PM": return 0.0
    try: return float(s)
    except Exception: return 0.0

# ===================== Lectura de archivos =====================
def read_lista(upload) -> Optional[pd.DataFrame]:
    """
    Lee la LISTA DE CODIGO desde CSV/XLS/XLSX.
    - Si es Excel: busca la hoja cuyo nombre contenga 'asignar_DOS√çMETRO' (ignorando may/min).
    - Normaliza columnas clave.
    Campos esperados/√∫tiles:
      C√ìDIGO_DOS√çMETRO, PERIODO DE LECTURA, C√ìDIGO DE USUARIO, NOMBRE, COMPA√ë√çA, TIPO DE DOS√çMETRO, C√âDULA
    """
    if not upload:
        return None

    name = upload.name.lower()
    df = None
    if name.endswith((".xlsx", ".xls")):
        xls = pd.ExcelFile(upload)
        sheet = None
        for s in xls.sheet_names:
            if "asignar" in s.lower() and "dos" in s.lower():
                sheet = s; break
        if sheet is None:
            sheet = xls.sheet_names[0]
        df = pd.read_excel(xls, sheet_name=sheet)
    else:
        # CSV: intenta distintas codificaciones y separadores
        raw = upload.read(); upload.seek(0)
        tried = [
            dict(sep=";", engine="python", encoding="utf-8-sig"),
            dict(sep=None, engine="python", encoding="utf-8-sig"),
            dict(sep=None, engine="python", encoding="latin-1"),
            dict(sep=",", engine="python", encoding="utf-8-sig"),
        ]
        for opts in tried:
            try:
                df = pd.read_csv(BytesIO(raw), **opts); break
            except Exception:
                continue
        if df is None:
            df = pd.read_csv(BytesIO(raw))
    if df is None or df.empty:
        return None

    # Normalizaci√≥n nombres
    df.columns = [str(c).strip() for c in df.columns]

    # Acomoda alias de columnas
    rename_map = {}
    for c in df.columns:
        cl = c.lower()
        if "c√≥digo" in cl and "dos" in cl:
            rename_map[c] = "C√ìDIGO_DOS√çMETRO"
        elif "codigo" in cl and "dos" in cl:
            rename_map[c] = "C√ìDIGO_DOS√çMETRO"
        elif cl in ("periodo de lectura", "per√≠odo de lectura", "periodo", "per√≠odo"):
            rename_map[c] = "PERIODO DE LECTURA"
        elif "c√≥digo de usuario" in cl or "codigo de usuario" in cl or cl == "codigo usuario":
            rename_map[c] = "C√ìDIGO DE USUARIO"
        elif cl == "compa√±√≠a" or cl == "compania":
            rename_map[c] = "COMPA√ë√çA"
        elif "tipo de dos" in cl:
            rename_map[c] = "TIPO DE DOS√çMETRO"
        elif cl == "cedula" or cl == "c√©dula":
            rename_map[c] = "C√âDULA"
    if rename_map:
        df = df.rename(columns=rename_map)

    needed = ["C√ìDIGO_DOS√çMETRO", "PERIODO DE LECTURA"]
    for c in needed:
        if c not in df.columns:
            df[c] = ""

    # Limpiezas y banderas
    df["C√ìDIGO_DOS√çMETRO"] = df["C√ìDIGO_DOS√çMETRO"].astype(str).str.strip().str.upper()
    df["PERIODO DE LECTURA"] = df["PERIODO DE LECTURA"].astype(str).str.strip().str.upper().str.replace(r"\.+$", "", regex=True)
    if "C√ìDIGO DE USUARIO" in df.columns:
        df["C√ìDIGO DE USUARIO"] = df["C√ìDIGO DE USUARIO"].astype(str).str.strip()

    # Nombre (si tienes NOMBRE+APELLIDO)
    if "NOMBRE" not in df.columns and "NOMBRE_COMPLETO" in df.columns:
        df["NOMBRE"] = df["NOMBRE_COMPLETO"]
    elif "NOMBRE" not in df.columns:
        df["NOMBRE"] = ""

    # Control flag
    def is_control_row(r):
        for k in ["NOMBRE", "C√âDULA", "C√ìDIGO DE USUARIO"]:
            if k in r and str(r[k]).strip().upper() == "CONTROL":
                return True
        return False
    df["IS_CONTROL"] = df.apply(is_control_row, axis=1)

    return df

def read_dosis(upload) -> Optional[pd.DataFrame]:
    if not upload:
        return None
    name = upload.name.lower()

    if name.endswith((".xlsx", ".xls")):
        df = pd.read_excel(upload)
    else:
        raw = upload.read(); upload.seek(0)
        tried = [
            dict(sep=";", engine="python", encoding="utf-8-sig"),
            dict(sep=None, engine="python", encoding="utf-8-sig"),
            dict(sep=None, engine="python", encoding="latin-1"),
            dict(sep=",", engine="python", encoding="utf-8-sig"),
        ]
        df = None
        for opts in tried:
            try:
                df = pd.read_csv(BytesIO(raw), **opts); break
            except Exception:
                continue
        if df is None:
            df = pd.read_csv(BytesIO(raw))

    if df is None or df.empty:
        return None

    # Normaliza columnas
    norm = (
        df.columns.astype(str)
        .str.strip().str.lower()
        .str.replace(" ", "", regex=False)
        .str.replace("(", "").str.replace(")", "")
        .str.replace(".", "", regex=False)
    )
    df.columns = norm

    # dosimeter
    if "dosimeter" not in df.columns:
        for alt in ["dosimetro", "codigo", "codigodosimetro", "codigo_dosimetro"]:
            if alt in df.columns:
                df.rename(columns={alt: "dosimeter"}, inplace=True); break

    # hp fields
    for cand in ["hp10dosecorr", "hp10dose", "hp10"]:
        if cand in df.columns:
            df.rename(columns={cand: "hp10dose"}, inplace=True); break
    for cand in ["hp007dosecorr", "hp007dose", "hp007"]:
        if cand in df.columns:
            df.rename(columns={cand: "hp0.07dose"}, inplace=True); break
    for cand in ["hp3dosecorr", "hp3dose", "hp3"]:
        if cand in df.columns:
            df.rename(columns={cand: "hp3dose"}, inplace=True); break

    # Num√©ricos
    for k in ["hp10dose", "hp0.07dose", "hp3dose"]:
        if k in df.columns:
            df[k] = pd.to_numeric(df[k], errors="coerce").fillna(0.0)
        else:
            df[k] = 0.0

    if "dosimeter" in df.columns:
        df["dosimeter"] = df["dosimeter"].astype(str).str.strip().str.upper()

    # timestamp opcional
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")

    return df

# ===================== UI: Secci√≥n 1 ‚Äî Cargar LISTA DE C√ìDIGO =====================
st.subheader("1) Cargar LISTA DE C√ìDIGO")
upl_lista = st.file_uploader("Subir LISTA DE C√ìDIGO (CSV/XLS/XLSX) ‚Äî si es Excel, la hoja puede llamarse 'asignar_DOS√çMETRO...'", type=["csv", "xlsx", "xls"])
df_lista = read_lista(upl_lista) if upl_lista else None
if df_lista is not None and not df_lista.empty:
    st.session_state.df_lista = df_lista.copy()
    st.success(f"LISTA cargada: {len(df_lista)} fila(s)")
    st.dataframe(df_lista.head(20), use_container_width=True)
else:
    st.info("LISTA vac√≠a o sin datos")

# ===================== UI: Secci√≥n 2 ‚Äî Cargar dosis =====================
st.subheader("2) Subir Archivo de Dosis")
upl_dosis = st.file_uploader("Selecciona CSV/XLS/XLSX (dosis)", type=["csv", "xls", "xlsx"], key="upl_dosis")
df_dosis = read_dosis(upl_dosis) if upl_dosis else None
if df_dosis is not None and not df_dosis.empty:
    st.session_state.df_dosis = df_dosis.copy()
    st.success(f"Dosis cargadas: {len(df_dosis)} fila(s)")
    st.dataframe(df_dosis.head(20), use_container_width=True)
else:
    st.info("Sube un archivo de dosis para continuar")

# ===================== Filtros y cruce =====================
st.subheader("3) Cruce por PERIODO y C√ìDIGO_DOS√çMETRO ‚Üî dosimeter")
if st.session_state.df_lista is not None and st.session_state.df_dosis is not None:
    dfL = st.session_state.df_lista.copy()
    dfD = st.session_state.df_dosis.copy()

    # Periodos disponibles (de la LISTA)
    periods = sorted(dfL["PERIODO DE LECTURA"].dropna().astype(str).unique().tolist())
    sel_periods = st.multiselect("Filtrar por PERIODO DE LECTURA (elige uno o varios; vac√≠o = TODOS)", periods)
    if sel_periods:
        dfL = dfL[dfL["PERIODO DE LECTURA"].isin([p.upper() for p in sel_periods])]

    # Index para join por c√≥digo (√∫ltimo timestamp por c√≥digo en dosis)
    dfD = dfD.sort_values("timestamp")
    dfD_last = dfD.groupby("dosimeter", as_index=False).last()

    # Join por c√≥digo
    merged = dfL.merge(dfD_last, left_on="C√ìDIGO_DOS√çMETRO", right_on="dosimeter", how="left", indicator=True)
    misses = merged[merged["_merge"] == "left_only"][["C√ìDIGO_DOS√çMETRO", "PERIODO DE LECTURA"]]
    if not misses.empty:
        with st.expander("‚ö†Ô∏è C√≥digos de LISTA sin dosis (no encontrados en dosimeter)"):
            st.dataframe(misses, use_container_width=True)

    # Filtra solo coincidencias
    matched = merged[merged["_merge"] == "both"].copy()
    if matched.empty:
        st.warning("No hay coincidencias C√ìDIGO_DOS√çMETRO ‚Üî dosimeter (revisa periodos/c√≥digos).")
    else:
        st.success(f"Coincidencias: {len(matched)}")
        st.dataframe(matched.head(20), use_container_width=True)

        # ===================== Subida a Ninox (BASE DE DATOS) =====================
        st.subheader("4) Subir coincidencias ‚Üí Ninox (BASE DE DATOS)")

        subir_pm_como_texto = st.checkbox("Subir 'PM' como texto (si tu tabla tiene Hp como texto)", value=False)
        solo_debug_1 = st.checkbox("Solo 1 registro (debug)", value=False)

        # Campos en Ninox
        try:
            ninox_fields = ninox_get_fields(TABLE_BASE)
        except Exception as e:
            ninox_fields = set()
            st.error(f"No pude leer los campos de Ninox: {e}")

        map_fields = {
            "PERIODO DE LECTURA": "PERIODO DE LECTURA",
            "COMPA√ë√çA": "COMPA√ë√çA",
            "C√ìDIGO_DOS√çMETRO": "C√ìDIGO DE DOS√çMETRO",
            "C√ìDIGO DE USUARIO": "C√ìDIGO DE USUARIO",
            "NOMBRE": "NOMBRE",
            "C√âDULA": "C√âDULA",
            "TIPO DE DOS√çMETRO": "TIPO DE DOS√çMETRO",
            # Hp mapeados
            "hp10dose": "Hp (10)",
            "hp0.07dose": "Hp (0.07)",
            "hp3dose": "Hp (3)",
        }

        def cast_hp(v):
            if isinstance(v, str) and v.strip().upper() == "PM":
                return "PM" if subir_pm_como_texto else None
            try:
                return float(v)
            except Exception:
                return None

        rows = []
        it = matched.head(1).iterrows() if solo_debug_1 else matched.iterrows()
        for _, r in it:
            fields = {}
            for src, dst in map_fields.items():
                if ninox_fields and dst not in ninox_fields:
                    continue
                val = r.get(src, "")
                if dst in {"Hp (10)", "Hp (0.07)", "Hp (3)"}:
                    val = cast_hp(val)
                else:
                    val = "" if pd.isna(val) else str(val)
                fields[dst] = val
            rows.append({"fields": fields})

        if st.button("‚¨ÜÔ∏è Subir a BASE DE DATOS"):
            with st.spinner("Subiendo‚Ä¶"):
                res = ninox_insert_rows(TABLE_BASE, rows, batch_size=300)
            if res.get("ok"):
                st.success(f"‚úÖ Subido a Ninox: {res.get('inserted', 0)} registro(s)")
            else:
                st.error(f"‚ùå Error al subir: {res.get('error')}")

# ===================== TAB: REPORTE FINAL =====================
st.markdown("---")
st.header("üìä Reporte Final: suma por **C√ìDIGO DE USUARIO** (personas) y por **C√ìDIGO DE DOS√çMETRO** (CONTROL)")

# Cargar tabla BASE DE DATOS desde Ninox
try:
    base_records = ninox_fetch_all(TABLE_BASE)
    rows = []
    for rec in base_records:
        f = rec.get("fields", {}) or {}
        rows.append({
            "PERIODO DE LECTURA": f.get("PERIODO DE LECTURA",""),
            "COMPA√ë√çA": f.get("COMPA√ë√çA",""),
            "C√ìDIGO DE DOS√çMETRO": f.get("C√ìDIGO DE DOS√çMETRO",""),
            "C√ìDIGO DE USUARIO": f.get("C√ìDIGO DE USUARIO",""),
            "NOMBRE": f.get("NOMBRE",""),
            "C√âDULA": f.get("C√âDULA",""),
            "TIPO DE DOS√çMETRO": f.get("TIPO DE DOS√çMETRO",""),
            "Hp (10)": as_num(f.get("Hp (10)")),
            "Hp (0.07)": as_num(f.get("Hp (0.07)")),
            "Hp (3)": as_num(f.get("Hp (3)")),
        })
    base = pd.DataFrame(rows)
except Exception as e:
    base = pd.DataFrame()
    st.error(f"No pude leer BASE DE DATOS: {e}")

if base.empty:
    st.info("A√∫n no hay datos en BASE DE DATOS.")
else:
    # Normaliza
    base["PERIODO DE LECTURA"] = base["PERIODO DE LECTURA"].astype(str).str.upper().str.replace(r"\.+$", "", regex=True)
    base["NOMBRE"] = base["NOMBRE"].fillna("").astype(str).str.strip()
    base["C√âDULA"] = base["C√âDULA"].fillna("").astype(str).str.strip()
    base["COMPA√ë√çA"] = base["COMPA√ë√çA"].fillna("").astype(str).str.strip()
    base["C√ìDIGO DE USUARIO"] = base["C√ìDIGO DE USUARIO"].fillna("").astype(str).str.strip()
    base["C√ìDIGO DE DOS√çMETRO"] = base["C√ìDIGO DE DOS√çMETRO"].fillna("").astype(str).str.strip().upper()

    # Filtros
    col1, col2 = st.columns([2,2])
    with col1:
        per_opts = sorted(base["PERIODO DE LECTURA"].dropna().unique().tolist())
        per_sel = st.multiselect("Filtrar por PERIODO(S) (vac√≠o = todos)", per_opts)
    with col2:
        comp_opts = ["(todas)"] + sorted([c for c in base["COMPA√ë√çA"].dropna().unique().tolist() if c])
        comp_sel = st.selectbox("Filtrar por COMPA√ë√çA", comp_opts, index=0)

    df_rep = base.copy()
    if per_sel:
        df_rep = df_rep[df_rep["PERIODO DE LECTURA"].isin(per_sel)]
    if comp_sel != "(todas)":
        df_rep = df_rep[df_rep["COMPA√ë√çA"] == comp_sel]

    if df_rep.empty:
        st.warning("No hay filas con los filtros seleccionados.")
    else:
        # Marca CONTROL
        df_rep["IS_CONTROL"] = df_rep.apply(
            lambda r: str(r["NOMBRE"]).upper()=="CONTROL" or str(r["C√âDULA"]).upper()=="CONTROL", axis=1
        )

        # Personas ‚Üí agrupar por C√ìDIGO DE USUARIO
        grp_user = (df_rep[~df_rep["IS_CONTROL"]]
                    .groupby(["PERIODO DE LECTURA","C√ìDIGO DE USUARIO"], as_index=False)
                    .agg({
                        "COMPA√ë√çA":"first",
                        "NOMBRE":"first",
                        "C√âDULA":"first",
                        "Hp (10)":"sum",
                        "Hp (0.07)":"sum",
                        "Hp (3)":"sum"
                    }))
        grp_user["AGRUPACI√ìN"] = "POR USUARIO"

        # Control ‚Üí agrupar por C√ìDIGO DE DOS√çMETRO
        grp_ctrl = (df_rep[df_rep["IS_CONTROL"]]
                    .groupby(["PERIODO DE LECTURA","C√ìDIGO DE DOS√çMETRO"], as_index=False)
                    .agg({
                        "COMPA√ë√çA":"first",
                        "NOMBRE":"first",
                        "C√âDULA":"first",
                        "Hp (10)":"sum",
                        "Hp (0.07)":"sum",
                        "Hp (3)":"sum"
                    }))
        grp_ctrl["AGRUPACI√ìN"] = "CONTROL (POR DOS√çMETRO)"

        # Unimos y ordenamos un poco
        # Uniformar nombres de columnas para mostrar en una sola tabla
        grp_user = grp_user.rename(columns={"C√ìDIGO DE USUARIO":"CLAVE"})
        grp_ctrl = grp_ctrl.rename(columns={"C√ìDIGO DE DOS√çMETRO":"CLAVE"})
        reporte = pd.concat([grp_user, grp_ctrl], ignore_index=True)
        reporte = reporte[[
            "AGRUPACI√ìN","PERIODO DE LECTURA","CLAVE","COMPA√ë√çA","NOMBRE","C√âDULA",
            "Hp (10)","Hp (0.07)","Hp (3)"
        ]].sort_values(["AGRUPACI√ìN","PERIODO DE LECTURA","CLAVE"])

        st.success(f"Reporte generado: {len(reporte)} fila(s)")
        st.dataframe(reporte, use_container_width=True)

        # Descargas
        csv_bytes = reporte.to_csv(index=False).encode("utf-8-sig")
        st.download_button("‚¨áÔ∏è Descargar Reporte (CSV)", data=csv_bytes,
                           file_name=f"ReporteFinal_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                           mime="text/csv")

        def to_excel_bytes(df: pd.DataFrame, sheet_name="Reporte"):
            bio = BytesIO()
            with pd.ExcelWriter(bio, engine="openpyxl") as w:
                df.to_excel(w, index=False, sheet_name=sheet_name)
            bio.seek(0)
            return bio.read()

        xlsx_bytes = to_excel_bytes(reporte)
        st.download_button("‚¨áÔ∏è Descargar Reporte (Excel)",
                           data=xlsx_bytes,
                           file_name=f"ReporteFinal_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
