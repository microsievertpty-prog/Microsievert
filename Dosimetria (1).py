# -*- coding: utf-8 -*-
import io
import re
import unicodedata
import requests
import pandas as pd
import streamlit as st
from datetime import datetime
from io import BytesIO
from dateutil.parser import parse as dtparse
from typing import List, Dict, Any, Optional, Set

from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, Border, Side, PatternFill
from openpyxl.utils import get_column_letter

# ===================== NINOX CONFIG =====================
API_TOKEN   = "edf312a0-98b8-11f0-883e-db77626d62e5"
TEAM_ID     = "YrsYfTegptdZcHJEj"
DATABASE_ID = "ow1geqnkz00e"
BASE_URL    = "https://api.ninox.com/v1"

# Tablas: lee de LISTA DE CODIGO, sube/reporta a BASE DE DATOS
DEFAULT_BASE_TABLE_HINT   = "LISTA DE CODIGO"   # lectura
DEFAULT_REPORT_TABLE_HINT = "BASE DE DATOS"     # escritura y reportes

# ===================== STREAMLIT (global) =====================
st.set_page_config(page_title="Microsievert - Dosimetr√≠a", page_icon="üß™", layout="wide")
st.title("üß™ Sistema de Gesti√≥n de Dosimetr√≠a ‚Äî Microsievert")
st.caption("Ninox + VALOR‚àíCONTROL + Reporte Actual/Anual/Vida + Exportaci√≥n")

if "df_final" not in st.session_state:
    st.session_state.df_final = None

# ===================== Helpers comunes =====================
def ninox_headers():
    return {"Authorization": f"Bearer {API_TOKEN}", "Content-Type": "application/json"}

@st.cache_data(ttl=300, show_spinner=False)
def ninox_list_tables(team_id: str, db_id: str):
    url = f"{BASE_URL}/teams/{team_id}/databases/{db_id}/tables"
    r = requests.get(url, headers=ninox_headers(), timeout=30)
    r.raise_for_status()
    return r.json()

def ninox_resolve_table_id(team_id: str, db_id: str, table_hint: str) -> str:
    hint = (table_hint or "").strip()
    if hint and " " not in hint and len(hint) <= 8:
        return hint  # parece ID
    for t in ninox_list_tables(team_id, db_id):
        tname = str(t.get("name", "")).strip().lower()
        tid   = str(t.get("id", "")).strip()
        if tname == hint.lower() or tid == hint:
            return tid
    return hint  # deja que la API grite si no existe

def fetch_all_records(team_id: str, db_id: str, table_hint: str, page_size: int = 1000) -> List[dict]:
    """Lee TODOS los registros de Ninox (limit/skip con fallback perPage/offset)."""
    table_id = ninox_resolve_table_id(team_id, db_id, table_hint)
    base = f"{BASE_URL}/teams/{team_id}/databases/{db_id}/tables/{table_id}/records"
    out = []
    # Ruta principal: limit/skip
    try:
        skip = 0
        while True:
            r = requests.get(base, headers=ninox_headers(), params={"limit": page_size, "skip": skip}, timeout=60)
            if r.status_code == 404:
                raise FileNotFoundError(f"No existe la tabla '{table_hint}' (id resuelto '{table_id}').")
            r.raise_for_status()
            chunk = r.json()
            if not chunk: break
            out.extend(chunk)
            if len(chunk) < page_size: break
            skip += page_size
    except FileNotFoundError:
        raise
    except Exception:
        # Fallback: perPage/offset
        offset = 0
        while True:
            r = requests.get(base, headers=ninox_headers(), params={"perPage": page_size, "offset": offset}, timeout=60)
            r.raise_for_status()
            batch = r.json()
            if not batch: break
            out.extend(batch)
            if len(batch) < page_size: break
            offset += page_size
    return out

@st.cache_data(ttl=300, show_spinner=False)
def ninox_fetch_records(team_id: str, db_id: str, table_hint: str, page_size: int = 1000) -> pd.DataFrame:
    rows = [x.get("fields", {}) for x in fetch_all_records(team_id, db_id, table_hint, page_size)]
    df = pd.DataFrame(rows) if rows else pd.DataFrame()
    df.columns = [str(c) for c in df.columns]
    return df

def ninox_insert_records(team_id: str, db_id: str, table_hint: str, rows: list, batch_size: int = 400):
    table_id = ninox_resolve_table_id(team_id, db_id, table_hint)
    url = f"{BASE_URL}/teams/{team_id}/databases/{db_id}/tables/{table_id}/records"
    if not rows:
        return {"ok": True, "inserted": 0}
    inserted = 0
    for i in range(0, len(rows), batch_size):
        chunk = rows[i:i+batch_size]
        r = requests.post(url, headers=ninox_headers(), json=chunk, timeout=60)
        if r.status_code != 200:
            return {"ok": False, "inserted": inserted, "error": f"{r.status_code} {r.text}"}
        inserted += len(chunk)
    return {"ok": True, "inserted": inserted}

@st.cache_data(ttl=120, show_spinner=False)
def ninox_get_table_fields(team_id: str, db_id: str, table_hint: str):
    table_id = ninox_resolve_table_id(team_id, db_id, table_hint)
    info = ninox_list_tables(team_id, db_id)
    fields = set()
    for t in info:
        if str(t.get("id")) == str(table_id):
            cols = t.get("fields") or t.get("columns") or []
            for c in cols:
                name = c.get("name") if isinstance(c, dict) else None
                if name: fields.add(name)
            break
    return fields

# ===================== Normalizadores =====================
def _clean_code(s: str) -> str:
    """Deja solo A..Z y 0..9, elimina NBSP, guiones, espacios, tildes, etc."""
    s = (s or "")
    s = unicodedata.normalize("NFKD", str(s))
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = re.sub(r"[^A-Za-z0-9]", "", s)
    return s.upper().strip()

def _norm_period(s: str) -> str:
    s = (s or "").strip().upper()
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"\.+$", "", s)
    return s

def as_value(v: Any):
    if v is None: return ""
    s = str(v).strip().replace(",", ".")
    if s.upper() == "PM": return "PM"
    try: return float(s)
    except Exception: return s

def as_num(v: Any) -> float:
    if v is None: return 0.0
    s = str(v).strip().replace(",", ".")
    if s == "" or s.upper() == "PM": return 0.0
    try: return float(s)
    except Exception: return 0.0

def pm_or_sum(raws, numeric_sum) -> Any:
    if isinstance(raws, (list, tuple, set)): arr = list(raws)
    elif isinstance(raws, pd.Series):        arr = raws.tolist()
    elif raws in (None, ""):                 arr = []
    else:                                    arr = [raws]
    vals = [str(x).upper() for x in arr if str(x).strip() != ""]
    if vals and all(v == "PM" for v in vals): return "PM"
    try:
        total = float(numeric_sum)
        if pd.isna(total): total = 0.0
    except Exception:
        total = 0.0
    return float(f"{total:.2f}")

def merge_raw_lists(*vals):
    out: List[Any] = []
    for v in vals:
        if isinstance(v, (list, tuple, set)): out.extend(v)
        elif isinstance(v, pd.Series):        out.extend(v.tolist())
        elif v in (None, ""):                 ...
        else:                                 out.append(v)
    return out

# ===================== Lectura de dosis (archivo) =====================
def leer_dosis(upload):
    if not upload: return None
    name = upload.name.lower()
    if name.endswith(".csv"):
        try:    df = pd.read_csv(upload, delimiter=';', engine='python')
        except: 
            upload.seek(0)
            df = pd.read_csv(upload)
    else:
        df = pd.read_excel(upload)

    norm = (df.columns.astype(str).str.strip().str.lower()
            .str.replace(' ', '', regex=False)
            .str.replace('(', '').str.replace(')', '')
            .str.replace('.', '', regex=False))
    df.columns = norm

    if 'dosimeter' not in df.columns:
        for alt in ['dosimetro','codigo','codigodosimetro','codigo_dosimetro']:
            if alt in df.columns:
                df.rename(columns={alt:'dosimeter'}, inplace=True); break

    for cand in ['hp10dosecorr','hp10dose','hp10']:
        if cand in df.columns: df.rename(columns={cand:'hp10dose'}, inplace=True); break
    for cand in ['hp007dosecorr','hp007dose','hp007']:
        if cand in df.columns: df.rename(columns={cand:'hp0.07dose'}, inplace=True); break
    for cand in ['hp3dosecorr','hp3dose','hp3']:
        if cand in df.columns: df.rename(columns={cand:'hp3dose'}, inplace=True); break

    for k in ['hp10dose','hp0.07dose','hp3dose']:
        if k in df.columns: df[k] = pd.to_numeric(df[k], errors='coerce').fillna(0.0)
        else: df[k] = 0.0

    if 'dosimeter' in df.columns:
        df['dosimeter'] = df['dosimeter'].apply(_clean_code)

    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

    return df

# ===================== Normalizador LISTA DE CODIGO =====================
def normalize_lista_codigo(df_raw: pd.DataFrame) -> pd.DataFrame:
    df = df_raw.copy()
    needed = [
        "C√âDULA","C√ìDIGO USUARIO","NOMBRE","APELLIDO","FECHA DE NACIMIENTO",
        "CLIENTE","C√ìDIGO_CLIENTE","ETIQUETA","C√ìDIGO_DOS√çMETRO",
        "PERIODO DE LECTURA","TIPO DE DOS√çMETRO"
    ]
    for c in needed:
        if c not in df.columns: df[c] = ""

    df["NOMBRE"] = df["NOMBRE"].fillna("").astype(str).str.strip()
    ap = df["APELLIDO"].fillna("").astype(str).str.strip()
    df["NOMBRE_COMPLETO"] = (df["NOMBRE"] + " " + ap).str.strip().replace({"^$": ""}, regex=True)

    df["CODIGO"] = df["C√ìDIGO_DOS√çMETRO"].apply(_clean_code)
    df["CLIENTE"] = df["CLIENTE"].fillna("").astype(str).str.strip()
    df["PERIODO_NORM"] = df["PERIODO DE LECTURA"].apply(_norm_period)

    def is_control_row(r):
        for k in ["ETIQUETA","NOMBRE","C√âDULA","C√ìDIGO USUARIO"]:
            v = str(r.get(k, "")).strip().upper()
            if v == "CONTROL": return True
        return False
    df["CONTROL_FLAG"] = df.apply(is_control_row, axis=1)
    df["TIPO DE DOS√çMETRO"] = df["TIPO DE DOS√çMETRO"].fillna("").astype(str).str.strip()
    df["C√âDULA"] = df["C√âDULA"].fillna("").astype(str).str.strip()
    return df

# ===================== Construcci√≥n registros (desde LISTA DE CODIGO) =====================
def construir_registros_desde_lista_codigo(
    df_lista: pd.DataFrame,
    df_dosis: pd.DataFrame,
    periodo_filtro: str = "‚Äî TODOS ‚Äî",
    debug_matches: bool = False
) -> (List[Dict[str, Any]], dict):
    per_f = _norm_period(periodo_filtro)
    if per_f and per_f not in ("‚Äî TODOS ‚Äî","TODOS","TODAS"):
        base = df_lista[df_lista["PERIODO_NORM"] == per_f].copy()
    else:
        base = df_lista.copy()

    # √çndice por c√≥digo del CSV
    df_d = df_dosis.copy()
    df_d["dosimeter"] = df_d["dosimeter"].apply(_clean_code)
    idx = df_d.set_index("dosimeter")

    registros = []
    base = pd.concat([base[base["CONTROL_FLAG"]], base[~base["CONTROL_FLAG"]]], ignore_index=True)

    # Debug info
    dbg = {"ninox_codes_period": set(), "dose_codes": set(df_d["dosimeter"].unique()), "no_match_sample": []}

    for _, r in base.iterrows():
        cod = r["CODIGO"]
        if not cod or cod.lower() == "nan": continue
        dbg["ninox_codes_period"].add(cod)
        if cod not in idx.index:
            if len(dbg["no_match_sample"]) < 50:
                dbg["no_match_sample"].append(cod)
            continue

        d = idx.loc[cod]
        if isinstance(d, pd.DataFrame):
            d = d.sort_values(by="timestamp").iloc[-1]

        ts = d.get("timestamp", pd.NaT)
        fecha_str = pd.to_datetime(ts).strftime("%d/%m/%Y %H:%M") if pd.notna(ts) else ""

        registros.append({
            "PERIODO DE LECTURA": r["PERIODO_NORM"] or "",
            "CLIENTE": r["CLIENTE"],
            "C√ìDIGO DE DOS√çMETRO": cod,
            "NOMBRE": r["NOMBRE_COMPLETO"] or r["NOMBRE"],
            "C√âDULA": r["C√âDULA"],
            "FECHA DE LECTURA": fecha_str,
            "TIPO DE DOS√çMETRO": r["TIPO DE DOS√çMETRO"] or "CE",
            "Hp(10)": float(d.get("hp10dose", 0.0) or 0.0),
            "Hp(0.07)": float(d.get("hp0.07dose", 0.0) or 0.0),
            "Hp(3)": float(d.get("hp3dose", 0.0) or 0.0),
            "_IS_CONTROL": bool(r["CONTROL_FLAG"]),
        })

    # Si no hubo matches, reintenta sin filtro para ayudar a diagnosticar
    if not registros and per_f not in ("", "‚Äî TODOS ‚Äî", "TODOS", "TODAS"):
        base_all = df_lista.copy()
        for _, r in base_all.iterrows():
            cod = r["CODIGO"]
            if not cod or cod.lower() == "nan": continue
            if cod not in idx.index: continue
            d = idx.loc[cod]
            if isinstance(d, pd.DataFrame): d = d.sort_values(by="timestamp").iloc[-1]
            ts = d.get("timestamp", pd.NaT)
            fecha_str = pd.to_datetime(ts).strftime("%d/%m/%Y %H:%M") if pd.notna(ts) else ""
            registros.append({
                "PERIODO DE LECTURA": r.get("PERIODO DE LECTURA",""),
                "CLIENTE": r.get("CLIENTE",""),
                "C√ìDIGO DE DOS√çMETRO": cod,
                "NOMBRE": r.get("NOMBRE_COMPLETO") or r.get("NOMBRE",""),
                "C√âDULA": r.get("C√âDULA",""),
                "FECHA DE LECTURA": fecha_str,
                "TIPO DE DOS√çMETRO": r.get("TIPO DE DOS√çMETRO","") or "CE",
                "Hp(10)": float(d.get("hp10dose",0.0) or 0.0),
                "Hp(0.07)": float(d.get("hp0.07dose",0.0) or 0.0),
                "Hp(3)":  float(d.get("hp3dose",0.0) or 0.0),
                "_IS_CONTROL": bool(r.get("CONTROL_FLAG", False)),
            })
        if registros and debug_matches:
            st.info("No hubo matches con el periodo filtrado; se muestran coincidencias ignorando el periodo para diagn√≥stico.")

    # Orden: control primero
    registros.sort(key=lambda x: (not x.get("_IS_CONTROL", False), x.get("NOMBRE","")))
    for r in registros:
        r.pop("_IS_CONTROL", None)
    return registros, dbg

# ===================== Valor - Control =====================
def periodo_desde_fecha(periodo_str: str, fecha_str: str) -> str:
    per = _norm_period(periodo_str)
    if per and per != "CONTROL": return per
    if not fecha_str: return per or ""
    try:
        fecha = pd.to_datetime(fecha_str, dayfirst=True, errors="coerce")
        if pd.isna(fecha): return per or ""
        meses = ["ENERO","FEBRERO","MARZO","ABRIL","MAYO","JUNIO",
                 "JULIO","AGOSTO","SEPTIEMBRE","OCTUBRE","NOVIEMBRE","DICIEMBRE"]
        return f"{meses[fecha.month-1]} {fecha.year}"
    except Exception:
        return per or ""

def aplicar_valor_menos_control(registros):
    if not registros: return registros
    base10 = float(registros[0]['Hp(10)'])
    base07 = float(registros[0]['Hp(0.07)'])
    base3  = float(registros[0]['Hp(3)'])
    for i, r in enumerate(registros):
        r['PERIODO DE LECTURA'] = periodo_desde_fecha(r.get('PERIODO DE LECTURA', ''), r.get('FECHA DE LECTURA', ''))
        if i == 0:
            r['NOMBRE']   = "CONTROL"
            r['Hp(10)']   = f"{base10:.2f}"
            r['Hp(0.07)'] = f"{base07:.2f}"
            r['Hp(3)']    = f"{base3:.2f}"
        else:
            for key, base in [('Hp(10)', base10), ('Hp(0.07)', base07), ('Hp(3)', base3)]:
                diff = float(r[key]) - base
                r[key] = "PM" if diff < 0.005 else f"{diff:.2f}"
    return registros

# ===================== Excel simple VALOR-CONTROL =====================
def exportar_excel_simple_valor_control(df_final: pd.DataFrame) -> bytes:
    wb = Workbook(); ws = wb.active; ws.title = "REPORTE DE DOSIS"
    border = Border(left=Side(style='thin'), right=Side(style='thin'),
                    top=Side(style='thin'),  bottom=Side(style='thin'))

    ws['I1'] = f"Fecha de emisi√≥n: {datetime.now().strftime('%d/%m/%Y')}"
    ws['I1'].font = Font(size=10, italic=True)
    ws['I1'].alignment = Alignment(horizontal='right', vertical='top')

    ws.merge_cells('A5:J5')
    c = ws['A5']; c.value = 'REPORTE DE DOSIMETR√çA'
    c.font = Font(bold=True, size=14); c.alignment = Alignment(horizontal='center')

    headers = [
        'PERIODO DE LECTURA','CLIENTE','C√ìDIGO DE DOS√çMETRO','NOMBRE',
        'C√âDULA','FECHA DE LECTURA','TIPO DE DOS√çMETRO','Hp(10)','Hp(0.07)','Hp(3)'
    ]
    for i, h in enumerate(headers, 1):
        cell = ws.cell(row=7, column=i, value=h)
        cell.font = Font(bold=True); cell.alignment = Alignment(horizontal='center')
        cell.fill = PatternFill('solid', fgColor='DDDDDD'); cell.border = border

    start = 8
    for ridx, (_, row) in enumerate(df_final.iterrows()):
        for cidx, h in enumerate(headers, 1):
            val = row.get(h, "")
            cell = ws.cell(row=start + ridx, column=cidx, value=val)
            cell.alignment = Alignment(horizontal='center', wrap_text=True)
            cell.font = Font(size=10); cell.border = border

    for col in ws.columns:
        mx = max(len(str(c.value)) if c.value else 0 for c in col) + 2
        ws.column_dimensions[get_column_letter(col[0].column)].width = mx

    bio = io.BytesIO(); wb.save(bio); bio.seek(0)
    return bio.read()

# ===================== UI: Tabs =====================
tab1, tab2 = st.tabs(["üì• Carga, VALOR‚àíCONTROL y Subida", "üìä Reporte Actual / Anual / Vida"])

# ===================== TAB 1 =====================
with tab1:
    st.subheader("üì§ Cargar archivo de Dosis ‚Üî LISTA DE CODIGO (Ninox) ‚Üí subir a BASE DE DATOS")

    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuraci√≥n (TAB 1)")
        base_table_hint   = st.text_input("Tabla de lectura (personas/c√≥digos)", value=DEFAULT_BASE_TABLE_HINT, key="tab1_base")
        report_table_hint = st.text_input("Tabla de escritura (salida)", value=DEFAULT_REPORT_TABLE_HINT, key="tab1_report")
        periodo_filtro    = st.text_input("Filtro PERIODO DE LECTURA (ej. AGOSTO 2025)", value="AGOSTO 2025", key="tab1_per")
        subir_pm_como_texto = st.checkbox("Subir 'PM' como TEXTO (si Hp son texto en Ninox)", value=True, key="tab1_pm_texto")
        debug_uno = st.checkbox("Enviar 1 registro (debug)", value=False, key="tab1_debug")
        show_tables = st.checkbox("Mostrar tablas Ninox (debug)", value=False, key="tab1_show")
        show_code_debug = st.checkbox("Mostrar debug de c√≥digos", value=False, key="tab1_code_dbg")

    try:
        if show_tables:
            st.expander("Tablas Ninox (debug)").json(ninox_list_tables(TEAM_ID, DATABASE_ID))
        df_lista_raw = ninox_fetch_records(TEAM_ID, DATABASE_ID, base_table_hint)
        if df_lista_raw.empty:
            st.warning("No hay datos en LISTA DE CODIGO.")
            df_participantes = None
        else:
            df_participantes = normalize_lista_codigo(df_lista_raw)
            st.success(f"Conectado a Ninox. Tabla: {base_table_hint} ‚Äî filas: {len(df_participantes)}")
            st.dataframe(df_participantes.head(50), use_container_width=True)
    except Exception as e:
        st.error(f"Error leyendo {base_table_hint}: {e}")
        df_participantes = None

    st.markdown("#### Archivo de Dosis")
    upload = st.file_uploader("Selecciona CSV/XLS/XLSX", type=["csv","xls","xlsx"], key="tab1_upl")
    df_dosis = leer_dosis(upload) if upload else None
    if df_dosis is not None:
        st.caption(f"Vista previa dosis (normalizada) ‚Äî filas: {len(df_dosis)}")
        st.dataframe(df_dosis.head(20), use_container_width=True)

    col1, col2 = st.columns([1,1])
    with col1:
        nombre_reporte = st.text_input("Nombre archivo (sin extensi√≥n)",
                                       value=f"ReporteDosimetria_{datetime.now().strftime('%Y-%m-%d')}",
                                       key="tab1_name")
    with col2:
        btn_proc = st.button("‚úÖ Procesar", type="primary", use_container_width=True, key="tab1_btn_proc")

    if btn_proc:
        if df_participantes is None or df_participantes.empty:
            st.error("No hay filas en LISTA DE CODIGO.")
        elif df_dosis is None or df_dosis.empty:
            st.error("No hay datos de dosis.")
        elif 'dosimeter' not in df_dosis.columns:
            st.error("El archivo de dosis debe tener la columna 'dosimeter'.")
        else:
            with st.spinner("Procesando..."):
                registros, dbg = construir_registros_desde_lista_codigo(
                    df_participantes, df_dosis, periodo_filtro=periodo_filtro, debug_matches=show_code_debug
                )

                if show_code_debug:
                    with st.expander("Debug de coincidencias"):
                        st.write(f"C√≥digos en Ninox (periodo '{_norm_period(periodo_filtro)}'): {len(dbg['ninox_codes_period'])}")
                        st.write(f"C√≥digos en archivo de dosis: {len(dbg['dose_codes'])}")
                        if dbg["no_match_sample"]:
                            st.write("Ejemplos sin match (hasta 50):")
                            st.write(sorted(dbg["no_match_sample"]))

                if not registros:
                    st.warning("No hay coincidencias C√ìDIGO_DOS√çMETRO ‚Üî dosimeter (revisa periodo/c√≥digos).")
                else:
                    registros = aplicar_valor_menos_control(registros)
                    df_final = pd.DataFrame(registros)

                    # Limpieza
                    df_final['PERIODO DE LECTURA'] = (
                        df_final['PERIODO DE LECTURA'].astype(str)
                        .str.replace(r'\.+$', '', regex=True).str.strip()
                    )
                    if not df_final.empty:
                        df_final.loc[df_final.index.min(), 'NOMBRE'] = 'CONTROL'
                        df_final['NOMBRE'] = df_final['NOMBRE'].astype(str).str.replace(r'\.+$', '', regex=True).str.strip()

                    st.session_state.df_final = df_final
                    st.success(f"¬°Listo! Registros generados: {len(df_final)}")
                    st.dataframe(df_final, use_container_width=True)

                    try:
                        xlsx = exportar_excel_simple_valor_control(df_final)
                        st.download_button(
                            "‚¨áÔ∏è Descargar Excel (VALOR‚àíCONTROL)",
                            data=xlsx,
                            file_name=f"{(nombre_reporte.strip() or 'ReporteDosimetria')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            key="tab1_dl"
                        )
                    except Exception as e:
                        st.error(f"No se pudo generar Excel: {e}")

    st.markdown("---")
    st.subheader("‚¨ÜÔ∏è Subir TODO a Ninox (tabla BASE DE DATOS)")

    CUSTOM_MAP = {
        "PERIODO DE LECTURA": "PERIODO DE LECTURA",
        "CLIENTE": "CLIENTE",
        "C√ìDIGO DE DOS√çMETRO": "C√ìDIGO_DOS√çMETRO",  # aseg√∫rate de tener este nombre en Ninox
        "NOMBRE": "NOMBRE",
        "C√âDULA": "C√âDULA",
        "FECHA DE LECTURA": "FECHA DE LECTURA",
        "TIPO DE DOS√çMETRO": "TIPO DE DOS√çMETRO",
    }
    SPECIAL_MAP = {"Hp(10)": "Hp (10)", "Hp(0.07)": "Hp (0.07)", "Hp(3)": "Hp (3)"}

    def resolve_dest_name(col_name: str) -> str:
        if col_name in SPECIAL_MAP: return SPECIAL_MAP[col_name]
        if col_name in CUSTOM_MAP:  return CUSTOM_MAP[col_name]
        return col_name

    def _hp_value(v, as_text_pm=True):
        if isinstance(v, str) and v.strip().upper() == "PM":
            return "PM" if as_text_pm else None
        try: return float(v)
        except Exception: return v if v is not None else None

    def _to_str(v):
        if pd.isna(v): return ""
        if isinstance(v, (pd.Timestamp, )):
            return v.strftime("%Y-%m-%d %H:%M:%S")
        return str(v)

    if st.button("Subir TODO a Ninox (tabla BASE DE DATOS)", key="tab1_btn_upload"):
        df_final = st.session_state.df_final
        if df_final is None or df_final.empty:
            st.error("Primero pulsa 'Procesar'.")
        else:
            try:
                ninox_fields = ninox_get_table_fields(TEAM_ID, DATABASE_ID, DEFAULT_REPORT_TABLE_HINT)
                if not ninox_fields:
                    st.warning("No pude leer los campos de la tabla en Ninox. Verifica el nombre/ID.")
            except Exception as e:
                st.error(f"No se pudo leer el esquema de la tabla Ninox: {e}")
                ninox_fields = set()

            with st.expander("Campos detectados en Ninox"):
                st.write(sorted(ninox_fields))

            rows, skipped_cols = [], set()
            iterator = df_final.head(1).iterrows() if debug_uno else df_final.iterrows()

            for _, row in iterator:
                fields_payload = {}
                for col in df_final.columns:
                    dest = resolve_dest_name(col)
                    if ninox_fields and dest not in ninox_fields:
                        skipped_cols.add(dest); continue
                    val = row[col]
                    if dest in {"Hp (10)", "Hp (0.07)", "Hp (3)"}:
                        val = _hp_value(val, as_text_pm=subir_pm_como_texto)
                    else:
                        val = _to_str(val)
                    fields_payload[dest] = val
                rows.append({"fields": fields_payload})

            if debug_uno:
                st.caption("Payload (primer registro):")
                st.json(rows[:1])

            with st.spinner("Subiendo a Ninox..."):
                res = ninox_insert_records(TEAM_ID, DATABASE_ID, DEFAULT_REPORT_TABLE_HINT, rows, batch_size=300)

            if res.get("ok"):
                st.success(f"‚úÖ Subido a Ninox: {res.get('inserted', 0)} registro(s).")
                if skipped_cols:
                    st.info("Columnas omitidas por no existir en Ninox:\n- " + "\n- ".join(sorted(skipped_cols)))
                try:
                    df_check = ninox_fetch_records(TEAM_ID, DATABASE_ID, DEFAULT_REPORT_TABLE_HINT)
                    st.caption("Contenido reciente en BASE DE DATOS:")
                    st.dataframe(df_check.tail(len(rows)), use_container_width=True)
                except Exception:
                    pass
            else:
                st.error(f"‚ùå Error al subir: {res.get('error')}")
                if skipped_cols:
                    st.info("Revisa/crea en Ninox los campos omitidos:\n- " + "\n- ".join(sorted(skipped_cols)))

# ===================== TAB 2 =====================
with tab2:
    st.subheader("üìä Reporte ‚Äî Actual, Anual y de por Vida (por persona) desde Ninox BASE DE DATOS")

    def normalize_df(records_raw: List[dict]) -> pd.DataFrame:
        rows = []
        for r in records_raw:
            f = r.get("fields", {}) or {}
            rows.append({
                "_id": r.get("id"),
                "PERIODO DE LECTURA": f.get("PERIODO DE LECTURA"),
                "CLIENTE": f.get("CLIENTE"),
                "C√ìDIGO DE DOS√çMETRO": str(
                    f.get("C√ìDIGO_DOS√çMETRO") or f.get("C√ìDIGO DE DOS√çMETRO") or ""
                ).strip(),
                "NOMBRE": f.get("NOMBRE"),
                "C√âDULA": f.get("C√âDULA"),
                "FECHA DE LECTURA": f.get("FECHA DE LECTURA"),
                "TIPO DE DOS√çMETRO": f.get("TIPO DE DOS√çMETRO"),
                "Hp10_RAW":  as_value(f.get("Hp (10)")),
                "Hp007_RAW": as_value(f.get("Hp (0.07)")),
                "Hp3_RAW":   as_value(f.get("Hp (3)")),
                "Hp10_NUM":  as_num(f.get("Hp (10)")),
                "Hp007_NUM": as_num(f.get("Hp (0.07)")),
                "Hp3_NUM":   as_num(f.get("Hp (3)")),
            })
        df = pd.DataFrame(rows)
        df["FECHA_DE_LECTURA_DT"] = pd.to_datetime(
            df["FECHA DE LECTURA"].apply(
                lambda x: dtparse(str(x), dayfirst=True) if pd.notna(x) and str(x).strip() != "" else pd.NaT
            ), errors="coerce"
        )
        df["NOMBRE_NORM"] = df["NOMBRE"].fillna("").astype(str).str.strip()
        df["C√âDULA_NORM"] = df["C√âDULA"].fillna("").astype(str).str.strip()
        return df

    with st.spinner("Cargando datos desde Ninox (BASE DE DATOS)‚Ä¶"):
        base_records = fetch_all_records(TEAM_ID, DATABASE_ID, DEFAULT_REPORT_TABLE_HINT)
        base = normalize_df(base_records)

    if base.empty:
        st.warning("No hay registros en la tabla BASE DE DATOS.")
        st.stop()

    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuraci√≥n (TAB 2)")
        per_order = (base.groupby("PERIODO DE LECTURA")["FECHA_DE_LECTURA_DT"].max()
                    .sort_values(ascending=False).index.astype(str).tolist())
        per_valid = [p for p in per_order if _norm_period(p) != "CONTROL"]
        periodo_actual = st.selectbox("Periodo actual", per_valid, index=0 if per_valid else None, key="tab2_periodo")

        usar_anual_automatico = st.checkbox("ANUAL autom√°tico (mismo a√±o del periodo actual)", value=True, key="tab2_auto")

        periodos_anteriores = st.multiselect(
            "Periodos anteriores (solo si ANUAL autom√°tico est√° desmarcado)",
            [p for p in per_valid if p != periodo_actual],
            default=[per_valid[1]] if (len(per_valid) > 1) else [], key="tab2_prev"
        )
        comp_opts = ["(todas)"] + sorted(base["CLIENTE"].dropna().astype(str).unique().tolist())
        compania = st.selectbox("Cliente", comp_opts, index=0, key="tab2_comp")
        tipo_opts = ["(todos)"] + sorted(base["TIPO DE DOS√çMETRO"].dropna().astype(str).unique().tolist())
        tipo = st.selectbox("Tipo de dos√≠metro", tipo_opts, index=0, key="tab2_tipo")

        files = st.file_uploader("Archivos de dosis (para filtrar por C√ìDIGO DE DOS√çMETRO) ‚Äî Opcional",
                                 type=["csv","xlsx","xls"], accept_multiple_files=True, key="tab2_codes")

    df_company_type = base.copy()
    if compania != "(todas)":
        df_company_type = df_company_type[df_company_type["CLIENTE"].astype(str) == compania]
    if tipo != "(todos)":
        df_company_type = df_company_type[df_company_type["TIPO DE DOS√çMETRO"].astype(str) == tipo]

    if df_company_type.empty:
        st.warning("No hay registros que cumplan los filtros de Cliente y/o Tipo de dos√≠metro.")
        st.stop()

    def read_codes_from_files(files) -> Set[str]:
        codes: Set[str] = set()
        for f in files or []:
            raw = f.read(); f.seek(0)
            name = f.name.lower()
            try:
                if name.endswith((".xlsx", ".xls")):
                    df = pd.read_excel(BytesIO(raw))
                else:
                    df = None
                    for enc in ("utf-8-sig","latin-1"):
                        try:
                            df = pd.read_csv(BytesIO(raw), sep=None, engine="python", encoding=enc); break
                        except Exception: continue
                    if df is None: df = pd.read_csv(BytesIO(raw))
            except Exception:
                continue
            if df is None or df.empty: continue
            cand = None
            for c in df.columns:
                cl = str(c).lower()
                if any(k in cl for k in ["dosim","c√≥digo","codigo","wb","dos√≠metro","dosimetro"]):
                    cand = c; break
            if cand is None:
                for c in df.columns:
                    if df[c].astype(str).str.contains(r"^WB\d{5,}$", case=False, na=False).any():
                        cand = c; break
            if cand is None: cand = df.columns[0]
            codes |= set(df[cand].astype(str).map(_clean_code))
        return {c for c in codes if c}

    codes_filter: Optional[Set[str]] = read_codes_from_files(files) if files else None
    if codes_filter:
        st.success(f"C√≥digos detectados: {len(codes_filter)}")

    # Construcci√≥n DF ACTUAL
    keys = ["NOMBRE_NORM", "C√âDULA_NORM"]
    df_curr = df_company_type[_norm_period(base["PERIODO DE LECTURA"]) == _norm_period(periodo_actual)].copy()
    if df_curr.empty:
        # alternativa robusta por si el norm period difiere en blancos/puntos
        df_curr = df_company_type[base["PERIODO DE LECTURA"].astype(str).str.strip().str.upper() == _norm_period(periodo_actual)].copy()

    if codes_filter:
        df_curr = df_curr[df_curr["C√ìDIGO DE DOS√çMETRO"].map(_clean_code).isin(codes_filter)]

    if df_curr.empty:
        st.warning("No hay registros en el per√≠odo actual con los filtros seleccionados.")
        st.stop()

    df_curr = df_curr.sort_values("FECHA_DE_LECTURA_DT")
    personas_actual = set(zip(df_curr["NOMBRE_NORM"], df_curr["C√âDULA_NORM"]))

    df_all_for_people = df_company_type.copy()
    df_all_for_people["_pair"] = list(zip(df_all_for_people["NOMBRE_NORM"], df_all_for_people["C√âDULA_NORM"]))
    df_all_for_people = df_all_for_people[df_all_for_people["_pair"].isin(personas_actual)]

    if df_all_for_people.empty:
        st.warning("No se encontr√≥ historial para las personas detectadas en el per√≠odo actual.")
        st.stop()

    # ACTUAL
    gb_curr_sum = df_curr.groupby(keys, as_index=False).agg({
        "PERIODO DE LECTURA": "last",
        "CLIENTE": "last",
        "C√ìDIGO DE DOS√çMETRO": "last",
        "NOMBRE": "last",
        "C√âDULA": "last",
        "FECHA_DE_LECTURA_DT": "max",
        "TIPO DE DOS√çMETRO": "last",
        "Hp10_NUM": "sum",
        "Hp007_NUM": "sum",
        "Hp3_NUM": "sum",
    })
    gb_curr_raw = df_curr.groupby(keys).agg({
        "Hp10_RAW": list,
        "Hp007_RAW": list,
        "Hp3_RAW": list
    }).rename(columns={
        "Hp10_RAW": "Hp10_ACTUAL_RAW_LIST",
        "Hp007_RAW": "Hp007_ACTUAL_RAW_LIST",
        "Hp3_RAW":  "Hp3_ACTUAL_RAW_LIST"
    }).reset_index()

    out = gb_curr_sum.merge(gb_curr_raw, on=keys, how="left").rename(columns={
        "Hp10_NUM": "Hp10_ACTUAL_NUM_SUM",
        "Hp007_NUM": "Hp007_ACTUAL_NUM_SUM",
        "Hp3_NUM":  "Hp3_ACTUAL_NUM_SUM",
    })

    # ANUAL
    usar_anual_automatico = st.session_state.get("tab2_auto", True)
    if usar_anual_automatico:
        if df_curr["FECHA_DE_LECTURA_DT"].notna().any():
            current_year = int(df_curr["FECHA_DE_LECTURA_DT"].dt.year.mode().iloc[0])
        else:
            m = re.search(r"\b(20\d{2}|19\d{2})\b", str(periodo_actual))
            current_year = int(m.group(1)) if m else datetime.now().year

        df_same_year = df_all_for_people[df_all_for_people["FECHA_DE_LECTURA_DT"].dt.year == current_year].copy()
        df_prev_same_year = df_same_year[df_same_year["PERIODO DE LECTURA"].astype(str) != str(periodo_actual)].copy()
    else:
        sel = st.session_state.get("tab2_prev", [])
        df_prev_same_year = df_all_for_people[df_all_for_people["PERIODO DE LECTURA"].astype(str).isin(sel)].copy()

    if not df_prev_same_year.empty:
        gb_prev_sum = df_prev_same_year.groupby(keys).agg({
            "Hp10_NUM": "sum",
            "Hp007_NUM": "sum",
            "Hp3_NUM": "sum"
        }).rename(columns={
            "Hp10_NUM": "Hp10_PREV_NUM_SUM",
            "Hp007_NUM": "Hp007_PREV_NUM_SUM",
            "Hp3_NUM":  "Hp3_PREV_NUM_SUM",
        }).reset_index()

        gb_prev_raw = df_prev_same_year.groupby(keys).agg({
            "Hp10_RAW": list,
            "Hp007_RAW": list,
            "Hp3_RAW": list
        }).rename(columns={
            "Hp10_RAW": "Hp10_PREV_RAW_LIST",
            "Hp007_RAW": "Hp007_PREV_RAW_LIST",
            "Hp3_RAW":  "Hp3_PREV_RAW_LIST"
        }).reset_index()

        out = out.merge(gb_prev_sum, on=keys, how="left").merge(gb_prev_raw, on=keys, how="left")
    else:
        out["Hp10_PREV_NUM_SUM"] = 0.0
        out["Hp007_PREV_NUM_SUM"] = 0.0
        out["Hp3_PREV_NUM_SUM"]  = 0.0
        out["Hp10_PREV_RAW_LIST"] = [[]]*len(out)
        out["Hp007_PREV_RAW_LIST"] = [[]]*len(out)
        out["Hp3_PREV_RAW_LIST"]  = [[]]*len(out)

    # VIDA
    gb_life_sum = df_all_for_people.groupby(keys).agg({
        "Hp10_NUM": "sum",
        "Hp007_NUM": "sum",
        "Hp3_NUM": "sum"
    }).rename(columns={
        "Hp10_NUM": "Hp10_LIFE_NUM_SUM",
        "Hp007_NUM": "Hp007_LIFE_NUM_SUM",
        "Hp3_NUM":  "Hp3_LIFE_NUM_SUM",
    }).reset_index()

    gb_life_raw = df_all_for_people.groupby(keys).agg({
        "Hp10_RAW": list,
        "Hp007_RAW": list,
        "Hp3_RAW": list
    }).rename(columns={
        "Hp10_RAW": "Hp10_LIFE_RAW_LIST",
        "Hp007_RAW": "Hp007_LIFE_RAW_LIST",
        "Hp3_RAW":  "Hp3_LIFE_RAW_LIST"
    }).reset_index()

    out = out.merge(gb_life_sum, on=keys, how="left").merge(gb_life_raw, on=keys, how="left")

    # Rellenos + columnas finales
    for c in [
        "Hp10_ACTUAL_NUM_SUM","Hp007_ACTUAL_NUM_SUM","Hp3_ACTUAL_NUM_SUM",
        "Hp10_PREV_NUM_SUM","Hp007_PREV_NUM_SUM","Hp3_PREV_NUM_SUM",
        "Hp10_LIFE_NUM_SUM","Hp007_LIFE_NUM_SUM","Hp3_LIFE_NUM_SUM",
    ]:
        if c not in out.columns: out[c] = 0.0
        out[c] = out[c].fillna(0.0)

    out["Hp (10) ACTUAL"]   = out.apply(lambda r: pm_or_sum(r.get("Hp10_ACTUAL_RAW_LIST", []), r["Hp10_ACTUAL_NUM_SUM"]), axis=1)
    out["Hp (0.07) ACTUAL"] = out.apply(lambda r: pm_or_sum(r.get("Hp007_ACTUAL_RAW_LIST", []), r["Hp007_ACTUAL_NUM_SUM"]), axis=1)
    out["Hp (3) ACTUAL"]    = out.apply(lambda r: pm_or_sum(r.get("Hp3_ACTUAL_RAW_LIST",  []), r["Hp3_ACTUAL_NUM_SUM"]),  axis=1)

    out["Hp (10) ANUAL"] = out.apply(lambda r: pm_or_sum(
        merge_raw_lists(r.get("Hp10_ACTUAL_RAW_LIST"), r.get("Hp10_PREV_RAW_LIST")),
        float(r["Hp10_ACTUAL_NUM_SUM"]) + float(r["Hp10_PREV_NUM_SUM"])
    ), axis=1)
    out["Hp (0.07) ANUAL"] = out.apply(lambda r: pm_or_sum(
        merge_raw_lists(r.get("Hp007_ACTUAL_RAW_LIST"), r.get("Hp007_PREV_RAW_LIST")),
        float(r["Hp007_ACTUAL_NUM_SUM"]) + float(r["Hp007_PREV_NUM_SUM"])
    ), axis=1)
    out["Hp (3) ANUAL"] = out.apply(lambda r: pm_or_sum(
        merge_raw_lists(r.get("Hp3_ACTUAL_RAW_LIST"), r.get("Hp3_PREV_RAW_LIST")),
        float(r["Hp3_ACTUAL_NUM_SUM"]) + float(r["Hp3_PREV_NUM_SUM"])
    ), axis=1)

    out["Hp (10) VIDA"]   = out.apply(lambda r: pm_or_sum(r.get("Hp10_LIFE_RAW_LIST", []), r["Hp10_LIFE_NUM_SUM"]), axis=1)
    out["Hp (0.07) VIDA"] = out.apply(lambda r: pm_or_sum(r.get("Hp007_LIFE_RAW_LIST", []), r["Hp007_LIFE_NUM_SUM"]), axis=1)
    out["Hp (3) VIDA"]    = out.apply(lambda r: pm_or_sum(r.get("Hp3_LIFE_RAW_LIST",  []), r["Hp3_LIFE_NUM_SUM"]), axis=1)

    def fmt_fecha(dtval):
        if pd.isna(dtval): return ""
        try: return pd.to_datetime(dtval).strftime("%d/%m/%Y %H:%M")
        except Exception: return str(dtval)

    out["FECHA Y HORA DE LECTURA"] = out["FECHA_DE_LECTURA_DT"].apply(fmt_fecha)
    out["PERIODO DE LECTURA"] = st.session_state.get("tab2_periodo", "")

    out["__is_control"] = out["NOMBRE"].fillna("").astype(str).str.strip().str.upper().eq("CONTROL")
    out = out.sort_values(["__is_control","NOMBRE","C√âDULA"], ascending=[False, True, True])

    FINAL_COLS = [
        "PERIODO DE LECTURA","CLIENTE","C√ìDIGO DE DOS√çMETRO","NOMBRE","C√âDULA",
        "FECHA Y HORA DE LECTURA","TIPO DE DOS√çMETRO",
        "Hp (10) ACTUAL","Hp (0.07) ACTUAL","Hp (3) ACTUAL",
        "Hp (10) ANUAL","Hp (0.07) ANUAL","Hp (3) ANUAL",
        "Hp (10) VIDA","Hp (0.07) VIDA","Hp (3) VIDA",
    ]
    for c in FINAL_COLS:
        if c not in out.columns: out[c] = ""
    out = out[FINAL_COLS]

    st.markdown("#### Reporte final (vista previa)")
    st.dataframe(out, use_container_width=True, hide_index=True)

    # Descargas
    csv_bytes = out.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "‚¨áÔ∏è Descargar CSV (UTF-8 con BOM)",
        data=csv_bytes,
        file_name=f"reporte_dosimetria_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        key="tab2_dl_csv"
    )

    def to_excel_simple(df: pd.DataFrame, sheet_name="Reporte"):
        bio = BytesIO()
        with pd.ExcelWriter(bio, engine="openpyxl") as w:
            df.to_excel(w, index=False, sheet_name=sheet_name)
        bio.seek(0); return bio.getvalue()

    xlsx_simple = to_excel_simple(out)
    st.download_button(
        "‚¨áÔ∏è Descargar Excel (tabla simple)",
        data=xlsx_simple,
        file_name=f"reporte_dosimetria_tabla_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key="tab2_dl_xlsx_simple"
    )


